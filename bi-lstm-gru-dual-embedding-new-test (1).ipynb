{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"},{"sourceId":11650,"sourceType":"datasetVersion","datasetId":8327},{"sourceId":19053,"sourceType":"datasetVersion","datasetId":14154},{"sourceId":2798066,"sourceType":"datasetVersion","datasetId":1709138}],"dockerImageVersionId":25160,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU, Conv1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nprint(tf.__version__)\ntf.test.is_gpu_available(\n    cuda_only=False,\n    min_cuda_compute_capability=None\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-25T05:48:03.430737Z","iopub.execute_input":"2024-01-25T05:48:03.430981Z","iopub.status.idle":"2024-01-25T05:48:05.312751Z","shell.execute_reply.started":"2024-01-25T05:48:03.430938Z","shell.execute_reply":"2024-01-25T05:48:05.312044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# 停用词列表，可以根据需要进行扩展\nstopwords = [\"the\", \"and\", \"is\", \"on\", \"in\", \"if\", \"for\", \"a\", \"an\", \"of\", \"or\", \"to\", \"it\", \"you\", \"your\"]\n\ndef clean_text(text):\n    # Remove HTML tags\n    text = re.sub(r'<[^>]+>', '', text)\n    \n    text = text.replace('\\n', '')\n\n    # Remove web links\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n\n    # Remove special characters, punctuation marks, and newlines\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n\n    # Remove extra white spaces\n    text = re.sub(r'\\s+', ' ', text)\n\n    # Remove stopwords\n    text = ' '.join(word for word in text.split() if word.lower() not in stopwords)\n\n    return text.lower()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:05.314181Z","iopub.execute_input":"2024-01-25T05:48:05.314439Z","iopub.status.idle":"2024-01-25T05:48:05.320628Z","shell.execute_reply.started":"2024-01-25T05:48:05.314387Z","shell.execute_reply":"2024-01-25T05:48:05.319737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import plotly.express as px\n# import plotly.offline as pyo\n# import plotly.graph_objects as go\nimport re\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport os\nimport zipfile\nimport warnings\nimport random\nimport nltk\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet\n# Helper function to perform synonym replacement\nstopwords = [\"the\", \"and\", \"is\", \"on\", \"in\", \"if\", \"for\", \"a\", \"an\", \"of\", \"or\", \"to\", \"it\", \"you\", \"your\"]\ndef synonym_replacement(text, n=5):\n    words = text.split()\n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if word not in stopwords]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    for random_word in random_word_list:\n        synonyms = get_synonyms(random_word)\n        if len(synonyms) >= 1:\n            synonym = random.choice(synonyms)\n            new_words = [synonym if word == random_word else word for word in new_words]\n            num_replaced += 1\n        if num_replaced >= n:\n            break\n    new_text = ' '.join(new_words)\n    return new_text\n\ndef get_synonyms(word):\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for lemma in syn.lemmas():\n            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n            synonym = \"\".join([char for char in synonym if char.isalpha()])\n            synonyms.add(synonym)\n    if word in synonyms:\n        synonyms.remove(word)\n    return list(synonyms)\n\n# Helper function to perform random insertion\ndef random_insertion(text, n=3):\n    words = text.split()\n    new_words = words.copy()\n    for _ in range(n):\n        word_to_insert = get_random_word(text)\n        random_index = random.randint(0, len(new_words))\n        new_words.insert(random_index, word_to_insert)\n    new_text = ' '.join(new_words)\n    return new_text\n\ndef get_random_word(word_source):\n    # Replace with your method to get random words\n    word = random.choice(word_source) if word_source else None\n    if word:\n        synonyms = get_synonyms(word)\n        if synonyms:\n            return random.choice(synonyms)\n    return word  # Return the original word if no synonyms are found\n\n# Helper function to perform random deletion\ndef random_deletion(text, p=0.2):\n    words = text.split()\n    if len(words) == 1:\n        return text\n    new_words = [word for word in words if random.uniform(0, 1) > p]\n    if len(new_words) == 0:\n        return random.choice(words)\n    new_text = ' '.join(new_words)\n    return new_text","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:05.321817Z","iopub.execute_input":"2024-01-25T05:48:05.322200Z","iopub.status.idle":"2024-01-25T05:48:25.634502Z","shell.execute_reply.started":"2024-01-25T05:48:05.322143Z","shell.execute_reply":"2024-01-25T05:48:25.633789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_FILES = [\n        '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\n\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\nMAX_LEN = 220\n\n\nTEXT_COLUMN = 'comment_text'\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2024-01-25T05:48:25.635653Z","iopub.execute_input":"2024-01-25T05:48:25.635888Z","iopub.status.idle":"2024-01-25T05:48:25.640809Z","shell.execute_reply.started":"2024-01-25T05:48:25.635840Z","shell.execute_reply":"2024-01-25T05:48:25.640106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/d/julian3833/jigsaw-toxic-comment-classification-challenge/train.csv')\ntrain_df ","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:25.641981Z","iopub.execute_input":"2024-01-25T05:48:25.642191Z","iopub.status.idle":"2024-01-25T05:48:26.643331Z","shell.execute_reply.started":"2024-01-25T05:48:25.642149Z","shell.execute_reply":"2024-01-25T05:48:26.642574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#先拆分\nX_train, X_valid_test, Y_train, Y_valid_test = train_test_split(train_df, train_df , test_size = 0.2, random_state=42)\nX_test, X_valid,Y_test,Y_valid = train_test_split(X_valid_test, Y_valid_test , test_size = 0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:26.644893Z","iopub.execute_input":"2024-01-25T05:48:26.645197Z","iopub.status.idle":"2024-01-25T05:48:26.708511Z","shell.execute_reply.started":"2024-01-25T05:48:26.645137Z","shell.execute_reply":"2024-01-25T05:48:26.707931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#分割出來後準備放入wordnet\nX_train['total'] =X_train [['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)\nX_toxic_train = X_train [X_train['total'] != 0] #大概1.3萬筆","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:26.710040Z","iopub.execute_input":"2024-01-25T05:48:26.710358Z","iopub.status.idle":"2024-01-25T05:48:26.825701Z","shell.execute_reply.started":"2024-01-25T05:48:26.710299Z","shell.execute_reply":"2024-01-25T05:48:26.824948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = X_train[list_classes].values #label\nX_train = X_train[TEXT_COLUMN].astype(str)  #str\nY_train = y\n#---------------------------\ny = X_toxic_train[list_classes].values #label\nX_toxic_train = X_toxic_train[TEXT_COLUMN].astype(str)  #str\nY_toxic_train = y\n#---------------------------\ny = X_test[list_classes].values #label\nX_test = X_test[TEXT_COLUMN].astype(str)  #str\nY_test = y\n#----------------------------\ny = X_valid[list_classes].values #label\nX_valid = X_valid[TEXT_COLUMN].astype(str)  #str\nY_valid = y","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:26.827038Z","iopub.execute_input":"2024-01-25T05:48:26.827335Z","iopub.status.idle":"2024-01-25T05:48:26.877299Z","shell.execute_reply.started":"2024-01-25T05:48:26.827280Z","shell.execute_reply":"2024-01-25T05:48:26.876571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#同義詞替換\nfor index in X_toxic_train.index:\n    X_toxic_train[index]=synonym_replacement(X_toxic_train[index], n=3)\nX_train = pd.concat([X_toxic_train, X_train])\nY_train = np.concatenate([Y_toxic_train, Y_train], axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:26.878604Z","iopub.execute_input":"2024-01-25T05:48:26.878847Z","iopub.status.idle":"2024-01-25T05:48:37.391140Z","shell.execute_reply.started":"2024-01-25T05:48:26.878801Z","shell.execute_reply":"2024-01-25T05:48:37.390231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#同義詞替換\nfor index in X_toxic_train.index:\n    X_toxic_train[index]=synonym_replacement(X_toxic_train[index], n=3)\nX_train = pd.concat([X_toxic_train, X_train])\nY_train = np.concatenate([Y_toxic_train, Y_train], axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:37.392474Z","iopub.execute_input":"2024-01-25T05:48:37.392731Z","iopub.status.idle":"2024-01-25T05:48:43.785166Z","shell.execute_reply.started":"2024-01-25T05:48:37.392683Z","shell.execute_reply":"2024-01-25T05:48:43.784499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#隨機插入\n# for index in X_toxic_train.index:\n#     X_toxic_train[index]=random_insertion(X_toxic_train[index], n=3)\n# X_train = pd.concat([X_toxic_train, X_train])\n# Y_train = np.concatenate([Y_toxic_train, Y_train], axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:43.786278Z","iopub.execute_input":"2024-01-25T05:48:43.786508Z","iopub.status.idle":"2024-01-25T05:48:43.790120Z","shell.execute_reply.started":"2024-01-25T05:48:43.786461Z","shell.execute_reply":"2024-01-25T05:48:43.789300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#隨機插入\n# for index in X_toxic_train.index:\n#     X_toxic_train[index]=random_deletion(X_toxic_train[index], p=0.2)\n# X_train = pd.concat([X_toxic_train, X_train])\n# Y_train = np.concatenate([Y_toxic_train, Y_train], axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:43.791312Z","iopub.execute_input":"2024-01-25T05:48:43.791517Z","iopub.status.idle":"2024-01-25T05:48:43.801060Z","shell.execute_reply.started":"2024-01-25T05:48:43.791482Z","shell.execute_reply":"2024-01-25T05:48:43.800485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #進一步清理文本data\n# for index in X_train.index:\n#     X_train[index]=clean_text(X_train[index])\n# for index in X_test.index:\n#     X_test[index]=clean_text(X_test[index])\n# for index in X_valid.index:\n#     X_valid[index]=clean_text(X_valid[index])","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:43.802433Z","iopub.execute_input":"2024-01-25T05:48:43.802670Z","iopub.status.idle":"2024-01-25T05:48:43.810256Z","shell.execute_reply.started":"2024-01-25T05:48:43.802623Z","shell.execute_reply":"2024-01-25T05:48:43.809610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\n\n# def focal_loss(gamma=3., alpha=0.1): #舊的，這個真的可以\n#     def focal_loss_fixed(y_true, y_pred):\n#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n#         epsilon = K.epsilon()\n#         pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n#         pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n#         print(\"y_true: \",y_true)\n#         print(\"y_pred: \",y_pred)\n#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n\n#     return focal_loss_fixed\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef focal_loss(alpha=0.75, gamma=4):\n    def focal_loss_fixed(y_true, y_pred):\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n\n        focal = -alpha * K.pow(1. - y_pred, gamma) * K.log(y_pred)\n        non_focal = -(1. - alpha) * K.pow(y_pred, gamma) * K.log(1. - y_pred)\n\n        loss = y_true * focal + (1. - y_true) * non_focal\n\n        return K.sum(loss)\n    return focal_loss_fixed","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:43.811463Z","iopub.execute_input":"2024-01-25T05:48:43.811681Z","iopub.status.idle":"2024-01-25T05:48:43.821360Z","shell.execute_reply.started":"2024-01-25T05:48:43.811643Z","shell.execute_reply":"2024-01-25T05:48:43.820592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n\ndef build_model(embedding_matrix):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    #x = SpatialDropout1D(0.2)(x)\n\n    x1 = SpatialDropout1D(0.2)(x)\n\n    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences = True))(x1)\n    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n    \n    y = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences = True))(x1)\n    y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n\n    avg_pool1 = GlobalAveragePooling1D()(x)\n    max_pool1 = GlobalMaxPooling1D()(x)\n   \n    avg_pool2 = GlobalAveragePooling1D()(y)\n    max_pool2 = GlobalMaxPooling1D()(y)\n   \n    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n\n    x = Dense(6, activation = \"sigmoid\")(x)\n\n    model = Model(inputs = words, outputs = x)\n\n    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n#     model.compile(loss=focal_loss(), optimizer=\"adam\", metrics=[\"accuracy\"])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-25T07:04:33.650150Z","iopub.execute_input":"2024-01-25T07:04:33.650762Z","iopub.status.idle":"2024-01-25T07:04:33.663762Z","shell.execute_reply.started":"2024-01-25T07:04:33.650699Z","shell.execute_reply":"2024-01-25T07:04:33.663007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(X_train)+list(X_test)+list(X_valid))\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(X_train, maxlen=MAX_LEN)\nX_test = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(X_test, maxlen=MAX_LEN)\nX_valid= tokenizer.texts_to_sequences(X_valid)\nX_valid = sequence.pad_sequences(X_valid, maxlen=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:48:43.835217Z","iopub.execute_input":"2024-01-25T05:48:43.835506Z","iopub.status.idle":"2024-01-25T05:49:17.181776Z","shell.execute_reply.started":"2024-01-25T05:48:43.835445Z","shell.execute_reply":"2024-01-25T05:49:17.181069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T05:49:17.182909Z","iopub.execute_input":"2024-01-25T05:49:17.183128Z","iopub.status.idle":"2024-01-25T05:58:03.313134Z","shell.execute_reply.started":"2024-01-25T05:49:17.183090Z","shell.execute_reply":"2024-01-25T05:58:03.312226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, accuracy_score\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\n\nEPOCHS = 4\nSEEDS = 4\n\noverall_accuracy = 0\nall_paramater=[[0.,0.,0.,0.,0.,0.],[0.,0.,0.,0.,0.,0.],[0.,0.,0.,0.,0.,0.],[0.,0.,0.,0.,0.,0.],[0.,0.,0.,0.,0.,0.],[0.,0.,0.,0.,0.,0.]]  #2D-list\nfor ii in range(SEEDS):\n    model = build_model(embedding_matrix)\n    for global_epoch in range(EPOCHS):\n        print(global_epoch)\n        model.fit(\n            X_train,\n            Y_train,\n            validation_data=(X_valid, Y_valid),\n            batch_size=128,\n            epochs=1,\n            verbose=2,\n            callbacks=[\n                LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n            ]\n        )\n        val_preds = model.predict(X_valid)\n        AUC = 0\n        for i in range(6):\n            AUC += roc_auc_score(Y_valid[:, i], val_preds[:, i]) / 6. \n        print(\"Validation AUC:\", AUC)\n    \n    test_preds=model.predict(X_test)\n    for i in range(6):\n        \n        y_true = Y_test[:, i]\n        y_pred = (test_preds[:, i] > 0.5).astype(int)\n    \n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n        # 計算 true negative rate\n        tnr = tn / (tn + fp)\n        all_paramater[i][0]+=tnr\n        \n        # 計算 precision\n        precision = tp / (tp + fp)\n        all_paramater[i][1]+=precision \n        \n        # 計算 recall\n        recall = tp / (tp + fn)\n        all_paramater[i][2]+=recall\n        \n        # 計算特異度（Specificity）\n        specificity = tn / (tn + fp)\n        all_paramater[i][3]+=specificity \n\n        # 計算 F1 分數\n        f1 = f1_score(y_true, y_pred)\n        all_paramater[i][4]+=f1 \n        \n        accuracy = (tp + tn) / (tp + tn + fp + fn)\n        all_paramater[i][5]+=accuracy \n        \n        print(f\"Class {i + 1} - True Negative Rate: {tnr}, Precision: {precision}, Recall: {recall}, Specificity: {specificity}, F1 Score: {f1}, Accuracy: {accuracy}\")\n\nprint(\"\\n\\n\")\nfor i in range(6):\n    print(f\"AVG: Class {i +1} - True Negative Rate:{all_paramater[i][0]/SEEDS},Precision:{all_paramater[i][1]/SEEDS},Recall:{all_paramater[i][2]/SEEDS},Specificity:{all_paramater[i][3]/SEEDS},F1 Score:{all_paramater[i][4]/SEEDS},Accuracy:{all_paramater[i][5]/SEEDS}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-25T07:04:40.396188Z","iopub.execute_input":"2024-01-25T07:04:40.396484Z","iopub.status.idle":"2024-01-25T07:39:50.875933Z","shell.execute_reply.started":"2024-01-25T07:04:40.396440Z","shell.execute_reply":"2024-01-25T07:39:50.875196Z"},"trusted":true},"execution_count":null,"outputs":[]}]}